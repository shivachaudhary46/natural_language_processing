{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Algorithm to see how WordPiece Tokenization works\n",
        "The algorithm follows a data driven approach to build its vocaulary. it starts with individual characters and gradually merges the most frequently occuring pairs until reaching a target a vocaulary size.\n",
        "\n",
        "- Take input text.\n",
        "- convert text into bigrams chars\n",
        "- calculate the unique co-occurency of each bigrams count.\n",
        "- Pick a consecutive pair that has highest joint probability\n",
        "  - p(c1, c2) = count(c1, c2) / count(1) * count(2)\n",
        "- treat previously picked pair as single token and add to vocabulary\n",
        "- pick a consecutive pair which have highest number of co occurency\n",
        "- treat previously picked pair as a single token and add to vocabulary. and repeat step 6"
      ],
      "metadata": {
        "id": "LEc56Vty_HJr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmdGLdHOw4SB",
        "outputId": "707be49d-0fb4-44b0-f30c-a1fcb7423785"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['h', 'u', 'g', 's', 'p', 'u', 'g', 'b', 'u', 'n', 'b', 'u', 'n', 'p', 'u', 'g', 'h', 'u', 'g', 's', 'c', 'h', 'u', 'n', 'k', 's', 'c', 'h', 'u', 'g', 's', 'r', 'u', 'g', 's']\n"
          ]
        }
      ],
      "source": [
        "# step 1 : take input text\n",
        "example_input = \"hugs pug bun bun pug hugs chunks chugs rugs\"\n",
        "example_input = list(example_input)\n",
        "new_example = [ i for i in example_input if i != \" \"]\n",
        "print(new_example)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_ngrams(text, n):\n",
        "    tokens = text\n",
        "    ngrams = [tuple(tokens[i:i + n]) for i in range(len(tokens) - n + 1)]\n",
        "    return ngrams\n",
        "\n",
        "bigrams = generate_ngrams(new_example, 2)\n",
        "\n",
        "print(\"Bigrams:\", bigrams)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4XrwgNrDjid",
        "outputId": "8446bf2f-76ef-4a1e-e0a1-5858c5e5ade9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bigrams: [('h', 'u'), ('u', 'g'), ('g', 's'), ('s', 'p'), ('p', 'u'), ('u', 'g'), ('g', 'b'), ('b', 'u'), ('u', 'n'), ('n', 'b'), ('b', 'u'), ('u', 'n'), ('n', 'p'), ('p', 'u'), ('u', 'g'), ('g', 'h'), ('h', 'u'), ('u', 'g'), ('g', 's'), ('s', 'c'), ('c', 'h'), ('h', 'u'), ('u', 'n'), ('n', 'k'), ('k', 's'), ('s', 'c'), ('c', 'h'), ('h', 'u'), ('u', 'g'), ('g', 's'), ('s', 'r'), ('r', 'u'), ('u', 'g'), ('g', 's')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "MtWEtaHgEv-D",
        "outputId": "6af31d9b-f9c9-479a-c7a9-6667a9989857"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-1885169875.py, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-1885169875.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    def\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    }
  ]
}