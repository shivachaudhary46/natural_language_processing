{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Dictionary-Based Tokenization\n",
        "  Dictionary-based tokenization is a type of tokenization where text is split into tokens using a predefined dictionary of words or phrases.\n",
        "\n",
        "***Why use dictionary-based tokenization when there are many other ways to create tokens?***\n",
        "\n",
        "Let’s say you have the text:\n",
        "\n",
        "\"I live in San Francisco\", or \"United Nations\".\n",
        "\n",
        "If you use simple whitespace tokenization, \"San Francisco\" will be split into two separate tokens: \"San\" and \"Francisco\". This can be a problem because:\n",
        "\n",
        "The original meaning of the named entity (city name) gets broken.\n",
        "\n",
        "The number of unique tokens increases (data sparsity), which will increase computational power.\n",
        "\n",
        "To solve such issues, dictionary-based tokenization helps by keeping important multi-word expressions together as a single token — for example, \"San Francisco\" stays as \"San_Francisco\".\n",
        "\n",
        "***How to perform dictionary-based tokenization?***\n",
        "\n",
        "Create a predefined dictionary of important words or phrases (e.g., place names, organizations, domain-specific terms).\n",
        "\n",
        "Preprocess the text (lowercasing, cleaning, etc.) and apply simple tokenization as the first step.\n",
        "\n",
        "Match tokens against the dictionary — if a sequence matches, treat it as a single token and assign a specific token ID or representation.\n",
        "\n",
        "Handle unmatched words — for words not found in the dictionary, apply subword or character-level tokenization to handle unknowns.\n",
        "\n",
        "***how can i do dictionary tokenization ?***\n",
        "- step 1: creating the predifined dictionary like creating the\n",
        "- step 2 : preprocess the text and do simple tokneization\n",
        "- step 3 : if the words matches to the dictionary then we give specific values to that words\n",
        "- step 4: if the words does not matches then the dictionary we would do subword or character tokenization"
      ],
      "metadata": {
        "id": "dn-QXnykXA4v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NTljdskUIqgJ"
      },
      "outputs": [],
      "source": [
        "## Prepairing the text\n",
        "predefined = [('San', 'Francisco'), ('United', 'Nations'), ('New', 'York'), ('Google', 'coolab')]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Importing the Libraries\n",
        "import nltk\n",
        "from nltk.tokenize import MWETokenizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88Yq29UEgejN",
        "outputId": "4c2f9c85-585f-49fe-dc5e-dca6cacb698f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dDtDslZtazOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing the Text\n",
        "def preprocess_text(text):\n",
        "  text = text.lower()\n",
        "  return text\n",
        "\n",
        "sample_text = 'San Francisco is a beautiful city. The United Nations meets regularly.'\n",
        "cleaned_text = preprocess_text(sample_text)"
      ],
      "metadata": {
        "id": "6xBhhzNshtoU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokrnizing the text\n",
        "def tokenizing(text):\n",
        "  tokenize = word_tokenize(text)\n",
        "  return tokenize\n",
        "  print('Tokenized text: ',tokenize)\n",
        "\n",
        "tokens=tokenizing(cleaned_text)\n",
        "print(tokenizing(cleaned_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TelCnPZ7iNHc",
        "outputId": "36d5b120-5fd4-4853-ab7d-a6f172271955"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['san', 'francisco', 'is', 'a', 'beautiful', 'city', '.', 'the', 'united', 'nations', 'meets', 'regularly', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying Dictionary based Tokenization\n",
        "tokenizer = MWETokenizer(predefined)\n",
        "tokenized_text = tokenizer.tokenize(tokens)\n",
        "print(\"Dictionary tokeized : \", tokenized_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KeC8oWk8ilcT",
        "outputId": "75976b5c-9237-4476-dcc1-e82e2155159d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dictionary tokeized :  ['san', 'francisco', 'is', 'a', 'beautiful', 'city', '.', 'the', 'united', 'nations', 'meets', 'regularly', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# taking out matched tokens\n",
        "unmatched_tokens = []\n",
        "for token in tokens:\n",
        "  if token not in ['San', 'Francisco', 'United', 'Nations']:\n",
        "    unmatched_tokens.append(token)\n",
        "\n",
        "print(\"Tokenized Sentence: \", unmatched_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCOGy4vpjiwc",
        "outputId": "976b34f6-5249-4fe1-81f5-77cfbfe163a2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized Sentence:  ['san', 'francisco', 'is', 'a', 'beautiful', 'city', '.', 'the', 'united', 'nations', 'meets', 'regularly', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def word_tokenizing(tokens):\n",
        "  unmatched_tokens = []\n",
        "  for token in tokens:\n",
        "    if token not in ['San', 'Francisco', 'United', 'Nations']:\n",
        "      unmatched_tokens.append(token)\n",
        "\n",
        "  return unmatched_tokens"
      ],
      "metadata": {
        "id": "v9W7ep2emOnZ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of Dictonary Based Tokenization in Action\n",
        "sentence = 'San Francisco is a part of the United Nations'\n",
        "tokens = word_tokenize(sentence)\n",
        "tokenized_sentence = word_tokenizing(tokens)\n",
        "print(tokenized_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-44cdg6lzy-",
        "outputId": "e1a2e718-4198-40ed-897e-a3f2c3667d5e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['is', 'a', 'part', 'of', 'the']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# customizing the dictionary\n",
        "predefined.extend([('Machine', 'Learning'), ('Natural', 'Learning' ,'processing')])\n",
        "tokenizer = MWETokenizer(predefined)"
      ],
      "metadata": {
        "id": "w-3YQ1KvlOaM"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing Tokenization Outpur\n",
        "sentences = [\n",
        "    'San Francisco is a beautiful place.',\n",
        "    'The United Nations is headquartered in New York.',\n",
        "    'Machine Learning is a subset of Artificial.'\n",
        "]\n",
        "\n",
        "for sentence in sentences:\n",
        "  cleaned_sentence = preprocess_text(sentence)\n",
        "  tokens = word_tokenize(cleaned_sentence)\n",
        "  tokenized_sentence = tokenizer.tokenize(tokens)\n",
        "  print('Original: ', sentence)\n",
        "  print('Tokenized: ', tokenized_sentence)\n",
        "  print('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8QES3uttpmhm",
        "outputId": "7135d6a5-a972-4512-c644-2cddc8dac8e5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:  San Francisco is a beautiful place.\n",
            "Tokenized:  ['san', 'francisco', 'is', 'a', 'beautiful', 'place', '.']\n",
            "\n",
            "\n",
            "Original:  The United Nations is headquartered in New York.\n",
            "Tokenized:  ['the', 'united', 'nations', 'is', 'headquartered', 'in', 'new', 'york', '.']\n",
            "\n",
            "\n",
            "Original:  Machine Learning is a subset of Artificial.\n",
            "Tokenized:  ['machine', 'learning', 'is', 'a', 'subset', 'of', 'artificial', '.']\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}