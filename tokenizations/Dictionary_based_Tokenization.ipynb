{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nN0uH8yP8pA0"
      },
      "outputs": [],
      "source": [
        "'''In Natural language Processing (NLP), dictionary-based tokenization is the process in which the text is split into tokens\n",
        "using a predefined dictionary of words, phrases and expressions. This method is very useful when we are using  multi words expression\n",
        "or domain specific phrases as a single token.\n",
        "\n",
        "for example if we got the words , New York if we do normal tokenization then it will seperate two words like \"New\" and \"york\". dictionary based tokenization\n",
        "make this possible. unlike a regular word tokenization which simply splits a sentence into invidiual words, it groups specific term\n",
        "together based on dictionary. This means that  differen single unit is treated a single unit which is important for many NLP task.\n",
        "'''\n",
        "\n",
        "'''\n",
        "For example: San Francisco is a beautiful city\n",
        "first the word in the phrases are search in the vocabulary entry if we got grasp of that word\n",
        "then , it s grouped as token, in above case \"san francisco is taken as token\"\n",
        "if we don't find the any word into the token then that word is tokenized into subword tokens or characters tokens so\n",
        "we can get grasp of the\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Steps for implementing Dictionary Based Tokenization\n"
      ],
      "metadata": {
        "id": "kO_trGNYEg4f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Importing required libraries"
      ],
      "metadata": {
        "id": "ch9aJvbAE0nc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import MWETokenizer\n",
        "import numpy as np\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LsuKoBmzEUZl",
        "outputId": "659f2186-eba1-4159-ae2a-b18ddfea9a34"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preparing the dictionary\n",
        "\n",
        "we will create a custom dictionary contining phrases that should be treated as single token like place names, organizations."
      ],
      "metadata": {
        "id": "XGyEmhuPFXK4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "custom_dict = [('San', 'Francisco'), ('New', 'York'), ('United', 'Nations')]"
      ],
      "metadata": {
        "id": "Vrv2v5u4FUDJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preprocessing the text\n",
        "now we preprocess the text into the lower case and we would not remove special character from the text. and after that tokenize into the tokens"
      ],
      "metadata": {
        "id": "hApV0tcSHu2d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing(text):\n",
        "  return text.lower()"
      ],
      "metadata": {
        "id": "E0m9bhAxt4Nw"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessAndTokenize(text):\n",
        "  process = text.lower()\n",
        "  token = word_tokenize(process)\n",
        "  return token\n",
        "\n",
        "sample_text = \"San francisco is a beautiul city. The United Nations meets regularly\"\n",
        "tokens = preprocessAndTokenize(sample_text)\n",
        "print(f\"Tokenzed text: {tokens}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0E6sedrEI71M",
        "outputId": "b99f49fa-32fc-401f-b510-73365c77577a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenzed text: ['san', 'francisco', 'is', 'a', 'beautiul', 'city', '.', 'the', 'united', 'nations', 'meets', 'regularly']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Applying dictionary based tokenization\n",
        "Now we apply MWETokenizer a Multi Word Expression Tokenizer in NLTK helps in grouping word expressions from the predefinded dicitionaruy into single tokens"
      ],
      "metadata": {
        "id": "nE6s0B-hKzbH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = MWETokenizer(custom_dict)\n",
        "tokenized_text = tokenizer.tokenize(tokens)\n",
        "print(\"dictionary based tokenzed text: \", tokenized_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i89tXo8RKP-G",
        "outputId": "842093db-f3cc-4cfc-fbca-8819cebd3ba5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dictionary based tokenzed text:  ['san', 'francisco', 'is', 'a', 'beautiul', 'city', '.', 'the', 'united', 'nations', 'meets', 'regularly']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# In the tokenization the words which are not found in dictionary remain as individual tokens . this helps in keeping the flexibility process while\n",
        "# keeping flexibility entact\n",
        "unmatched = [token for token in tokens if token not in ['united', 'nations', 'San', \"Francisco\"]]\n",
        "print(\"unmatched Tokens: \", unmatched)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zCrOGwYMozT",
        "outputId": "64371140-e25d-40ca-c2fd-cdb991514cb4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unmatched Tokens:  ['san', 'francisco', 'is', 'a', 'beautiul', 'city', '.', 'the', 'meets', 'regularly']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Bi-j55q4XyTB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}