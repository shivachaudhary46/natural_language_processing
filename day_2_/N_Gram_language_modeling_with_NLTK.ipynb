{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFSZ0UXqlnfE"
      },
      "source": [
        "# N-Gram Language Modeling Techniques\n",
        "\n",
        "### Introduction\n",
        "     Language Modeling involves determining the probability of a sequence of words. It is a fundamental to many Natural Lnaguage Processing (NLP). This technique is used in many application like Speech Recognition, Machine Translation, and spam Filtering where predicting or ranking the likelihood of phrases and sentences is crucial.\n",
        "\n",
        "#### For example:\n",
        "\n",
        "    - words like: \"This\", \"article\", \"is\", \"on\", \"NLP\" -> unigram\n",
        "    - if we convert it to bigram: \"This aricle\", \"article is\", \"is on\",  \"on NLP\"\n",
        "    - if we convert it to trigram: \"This article is\", \"article is on\", \"is on NLP\"\n",
        "\n",
        "It is said that N-gram model predicts the next word with previous word. for example: in trigram model, the next word is predicted by the previous two words.\n",
        "\n",
        "    goal:  calculate P(W | H), the probability to predict the next word W if we have history H.\n",
        "    Example:  For this phrase, \"I want to go Europe for\". if we want to predict next word of likelihood (0 - 1) then,\n",
        "\n",
        "#### For example :\n",
        "     P(\"studying\" | \"I\", \"want\", \"to\", \"go\", \"Europe\", \"for\")\n",
        "\n",
        "#### Chain Rule Of Probability\n",
        "    P(w1, w2, w3, w4, . . . , wn) = ∑ (wn | w1, w2, . . . , wn-1)\n",
        "\n",
        "#### Markov Assumption\n",
        "    To reduce complexity N - gram models assume the probability of a word depends only on the previous n-1 words.\n",
        "\n",
        "#### Evaluating Language Models\n",
        "    1. Entropy: measures the uncertainity of information content in a distribution.\n",
        "    H(p) =  ∑x P(x) . ( - log(P(x))\n",
        "\n",
        "    It always give non negative values.\n",
        "    2. Cross Entropy: Measures how well a probability distribution predicts a sample from test data.\n",
        "    H(p, q) = -  ∑x P(x) log(q(x))\n",
        "\n",
        "    3. Perpelexity : Exponential of cross - entropy; lower values indicate a better model.\n",
        "    PP=P(w1​,w2​,…,wN​)−N1​\n",
        "\n",
        "    Implementing N - Gram Language Modeling in NLTK\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Implementing N - Gram Language Modeling in NLTK \n",
        "1. words = nltk.word_tokenize(\"\".join(reuters.words())) tokenizes the entire Reuters corpus into words. \n",
        "2. tri_grams = list(trigrams(words))\n",
        "3. model[(w1, w2)][w3] += 1, counts occurences of third word w3 after (w1, w2)\n",
        "4. model[w1_w2][w3] /= total_count : converts counts to probabilities\n",
        "5. return the most likely next word based on highest probability. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "py312",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
