{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izuLxvSIxhbt",
        "outputId": "98c1e517-9c7b-48cc-e522-a5ef62350e1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting FastText\n",
            "  Downloading fasttext-0.9.3.tar.gz (73 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/73.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2 (from FastText)\n",
            "  Using cached pybind11-3.0.0-py3-none-any.whl.metadata (10.0 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from FastText) (75.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from FastText) (2.0.2)\n",
            "Using cached pybind11-3.0.0-py3-none-any.whl (292 kB)\n",
            "Building wheels for collected packages: FastText\n",
            "  Building wheel for FastText (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for FastText: filename=fasttext-0.9.3-cp311-cp311-linux_x86_64.whl size=4508434 sha256=6e9044be4445a38e49054737e2a17dcd05ace93f0c437da0dca175bfef14fb81\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/4f/35/5057db0249224e9ab55a513fa6b79451473ceb7713017823c3\n",
            "Successfully built FastText\n",
            "Installing collected packages: pybind11, FastText\n",
            "Successfully installed FastText-0.9.3 pybind11-3.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install FastText"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# step 1: installing the libraries\n",
        "import fasttext\n",
        "import os\n",
        "import joblib"
      ],
      "metadata": {
        "id": "Fz_Of8CN82x1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step 2: Creating Training Data\n",
        "def create_sample_data():\n",
        "  # sample sentences for training\n",
        "\n",
        "  number = int(input(\"enter the number of the strings you want to take as input: \"))\n",
        "  sentences = [\n",
        "      \"The king rules the kingdom\",\n",
        "      \"The queen helps the king\"\n",
        "  ]\n",
        "  for i in range(0, number):\n",
        "    sentence = input(f\"enter the sentence: {i+1}\")\n",
        "    sentences.append(sentence)\n",
        "\n",
        "  with open(\"training_data.txt\", \"w\") as f:\n",
        "    for sentence in sentences:\n",
        "      f.write(sentence.lower() + \"\\n\")\n",
        "\n",
        "  print(\"Training data created in 'training_data.txt'\")\n",
        "\n",
        "create_sample_data()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pfs9SMy4EY-Z",
        "outputId": "b4c8ed49-a28a-4bee-ebf0-dc67b9c6e377"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enter the number of the strings you want to take as input: 3\n",
            "enter the sentence: 1The cat is playing basketball\n",
            "enter the sentence: 2i am eating dinner\n",
            "enter the sentence: 3Can you give me football?\n",
            "Training data created in 'training_data.txt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dummy_data():\n",
        "  sentences = [\n",
        "      \"A cat is smiling\",\n",
        "      \"I am playing basketball\",\n",
        "      \"My mom is cooking food\",\n",
        "      \"My brother is saying rubbish things\"\n",
        "  ]\n",
        "\n",
        "  with open(\"training_data.txt\", \"a\") as f:\n",
        "    for sentence in sentences:\n",
        "      f.write(sentence.lower() + '\\n')\n",
        "\n",
        "  print(\"Training data appended in training_data.txt\")\n",
        "\n",
        "create_dummy_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckTmZDlPKj3c",
        "outputId": "eb63a1e5-ee79-4fbe-98e8-17b747e650fd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data created in training_data.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# step 3: Training a basic fast text model\n",
        "def train_skipgram_model():\n",
        "  model = fasttext.train_unsupervised(\"training_data.txt\", model=\"skipgram\", dim=50, epoch=30, minCount=1, minn=3, maxn=6)\n",
        "  '''\n",
        "    sample data are in training_data.txt\n",
        "    training on the skipgram model\n",
        "    choosing model as skipgram\n",
        "    dimension choosen as 50\n",
        "    epochs = 50\n",
        "    min word frequency of embedding is choosen as 1\n",
        "    minimum n gram is taken as 3\n",
        "    maximum n gram is taken as 6\n",
        "  '''\n",
        "  model.save_model(\"word_vectors.bin\")\n",
        "  print(\"Model trained and saved as 'word_vectors.bin'\")\n",
        "  return model\n",
        "\n",
        "train_skipgram_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZ9FtMyuHILX",
        "outputId": "b460ec09-fc59-4053-8bd9-d182a748192b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model trained and saved as 'word_vectors.bin'\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<fasttext.FastText._FastText at 0x7bae4535b150>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_cbow_model():\n",
        "  model = fasttext.train_unsupervised(\"training_data.txt\", model=\"cbow\", dim=50, minCount=1, minn=3, maxn=6)\n",
        "  \"\"\"\n",
        "    data is saved in the training_data.txt\n",
        "    this model is trained on using cbow model, so we can check skipgram is efficient or the skipgram\n",
        "    embedding dimension is set to the 50\n",
        "    min frequency is set to be 1\n",
        "    min n gram is 3\n",
        "    and max n gram is 6\n",
        "  \"\"\"\n",
        "  model.save_model(\"word_vectors_cbow.bin\")\n",
        "  print(\"Model trainied and saved as 'word_vectors.bin'\")\n",
        "  return model\n",
        "\n",
        "train_cbow_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqNNunrvxvk8",
        "outputId": "1e1125b7-76ab-41e5-81ee-8ef7b6e6fe59"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model trainied and saved as 'word_vectors.bin'\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<fasttext.FastText._FastText at 0x7bae282b6750>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_skipgram_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TiwdLIfJiMm",
        "outputId": "f10ce824-891b-47cb-c6eb-d4c2c51c745a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model trained and saved as 'word_vectors.bin'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_cbow = train_cbow_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmIKZt-e_x7Y",
        "outputId": "c27aaa2f-47f0-4095-fe79-ab7392ee3253"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model trainied and saved as 'word_vectors.bin'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# step 4: step getting word vecotrs\n",
        "def get_word_vectors(model):\n",
        "  king_vector = model.get_word_vector(\"king\")\n",
        "  print(f\"vector for 'king': {king_vector[:5]}\")\n",
        "  print(f\"vector shape: {king_vector.shape}\")\n",
        "\n",
        "  kingdom_vector = model.get_word_vector(\"kingdom\")\n",
        "  print(f\"vector for 'kingdom': {kingdom_vector[:5]}\")\n",
        "  print(f\"vector shape: {kingdom_vector.shape}\")\n",
        "\n",
        "print(f\"the word vector for king and kingdom by using the skipgram model\")\n",
        "get_word_vectors(model)\n",
        "print(\"\\n\")\n",
        "print(f\"the word vector for king and kingdom by using the cbow model\")\n",
        "get_word_vectors(model_cbow)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-Anm28LJu3a",
        "outputId": "f5c714ca-6803-43f4-d9a4-35c1680d9f50"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the word vector for king and kingdom by using the skipgram model\n",
            "vector for 'king': [-0.00019264 -0.00032311  0.00042608  0.00088622 -0.00164325]\n",
            "vector shape: (50,)\n",
            "vector for 'kingdom': [-0.00020329 -0.00094665  0.00032624 -0.00194728 -0.00075324]\n",
            "vector shape: (50,)\n",
            "\n",
            "\n",
            "the word vector for king and kingdom by using the cbow model\n",
            "vector for 'king': [-0.0001826  -0.00033079  0.0004302   0.00088911 -0.00164602]\n",
            "vector shape: (50,)\n",
            "vector for 'kingdom': [-0.00019653 -0.00095493  0.00033112 -0.00194544 -0.00075633]\n",
            "vector shape: (50,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# step 5: finding similar words\n",
        "def find_similar_words(model, word, k=3):\n",
        "  print(f\"\\n words similar to '{word}'\")\n",
        "  try:\n",
        "    neighbor_word = model.get_nearest_neighbors(word, k)\n",
        "    for i, (similarity, similar_words) in enumerate(neighbor_word):\n",
        "      print(f\"{i}. {similar_words} : {similarity}\")\n",
        "  except Exception as e:\n",
        "    print(f\"error: {e}\")\n",
        "\n",
        "find_similar_words(model, 'engineer')\n",
        "print(\"\\n\")\n",
        "find_similar_words(model, \"pochinki\")\n",
        "print(\"\\n\")\n",
        "find_similar_words(model_cbow, \"hello\")\n",
        "print(\"\\n\")\n",
        "find_similar_words(model_cbow, \"cats\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnEbpNF0Ls1K",
        "outputId": "e87025d3-5301-44dc-f26c-2dd0510ea315"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " words similar to 'engineer'\n",
            "0. mom : 0.3215050995349884\n",
            "1. smiling : 0.26361843943595886\n",
            "2. king : 0.15681423246860504\n",
            "\n",
            "\n",
            "\n",
            " words similar to 'pochinki'\n",
            "0. brother : 0.24800024926662445\n",
            "1. playing : 0.2303360104560852\n",
            "2. dinner : 0.21316547691822052\n",
            "\n",
            "\n",
            "\n",
            " words similar to 'hello'\n",
            "0. helps : 0.38231831789016724\n",
            "1. cat : 0.18260356783866882\n",
            "2. saying : 0.16983535885810852\n",
            "\n",
            "\n",
            "\n",
            " words similar to 'cats'\n",
            "0. eating : 0.2832822799682617\n",
            "1. smiling : 0.14695900678634644\n",
            "2. am : 0.14695431292057037\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# step 6 : text classification implementation\n",
        "def create_classification_data():\n",
        "  reviews = [\n",
        "      (\"This movie is amazing and fun\", \"Positive\"),\n",
        "      (\"I had really great time in meeting new people\", \"Positive\"),\n",
        "      (\"Excellent film with good plot\", \"Positive\"),\n",
        "      (\"Terrible movie very boring\", \"negative\"),\n",
        "      (\"Bad acting and poor story\", \"negative\"),\n",
        "      (\"Boring and predictable plot\", \"negative\")\n",
        "  ]\n",
        "\n",
        "  with open('movie_reviews.txt', 'w') as f:\n",
        "    for text, label in reviews:\n",
        "      f.write(f\"__label__{label} {text.lower()} \\n\")\n",
        "\n",
        "  print(\"Classification data created 'movie_reviews.txt'\")\n",
        "\n",
        "create_classification_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7Ci0qvAutFY",
        "outputId": "51ee36ab-9d53-427c-844c-605ed391e180"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification data created 'movie_reviews.txt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# training the text classifier\n",
        "def train_text_classifier():\n",
        "    classifier = fasttext.train_supervised(\n",
        "        'movie_reviews.txt',\n",
        "        epoch=25,\n",
        "        lr=0.1,\n",
        "        wordNgrams=2,\n",
        "        verbose=2\n",
        "    )\n",
        "    \"\"\"\n",
        "    read the data from the movie_reviewws.txt\n",
        "    epoch is 25\n",
        "    learning rate is used as 0.1 which is the magnitude of change/update to model weights during the backpropagation\n",
        "    training process. standard value for learning rate is used as less that 1.0\n",
        "    \"\"\"\n",
        "\n",
        "    classifier.save_model('text_classifier.bin')\n",
        "    print(\"Classifier trained and saved\")\n",
        "    return classifier\n",
        "\n",
        "classifier = train_text_classifier()\n",
        "\n"
      ],
      "metadata": {
        "id": "Ld1-0F-J0jFs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6ed25d7-8207-42a9-c0da-6d3ecc9b7bb5"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classifier trained and saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# making predictions using text_classifier\n",
        "def test_classifier(classifier):\n",
        "    test_sentences = [\n",
        "        \"This is a fantastic movie\",\n",
        "        \"Boring and terrible film\",\n",
        "        \"Great story and acting\",\n",
        "        \"Worst movie I have seen\"\n",
        "    ]\n",
        "\n",
        "    print(\"\\nClassification Results:\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    for sentence in test_sentences:\n",
        "        labels, probabilities = classifier.predict(sentence, k=1)\n",
        "        predicted_label = labels[0].replace('__label__', '')\n",
        "        confidence = probabilities[0]\n",
        "        print(f\"Text: '{sentence}'\")\n",
        "        print(f\"Prediction: {predicted_label} (confidence: {confidence:.4f})\\n\")\n",
        "\n",
        "test_classifier(classifier)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "fmG3YeoqGU-N",
        "outputId": "334454f1-65b8-4a0c-a6a3-d96db36cf7ad"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Results:\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Unable to avoid copy while creating an array as requested.\nIf using `np.array(obj, copy=False)` replace it with `np.asarray(obj)` to allow a copy when needed (no behavior change in NumPy 1.x).\nFor more details, see https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-886305265.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Prediction: {predicted_label} (confidence: {confidence:.4f})\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mtest_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-886305265.py\u001b[0m in \u001b[0;36mtest_classifier\u001b[0;34m(classifier)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_sentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mpredicted_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'__label__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mconfidence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobabilities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/fasttext/FastText.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, text, k, threshold, on_unicode_error)\u001b[0m\n\u001b[1;32m    237\u001b[0m                 \u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_input_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unable to avoid copy while creating an array as requested.\nIf using `np.array(obj, copy=False)` replace it with `np.asarray(obj)` to allow a copy when needed (no behavior change in NumPy 1.x).\nFor more details, see https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword."
          ]
        }
      ]
    }
  ]
}