{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Libraries"
      ],
      "metadata": {
        "id": "YNiFPgmzIeXR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JYj074I98iOd"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "Po4EdiliMAZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "g6lJBm-cMSuw"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Vocabulary"
      ],
      "metadata": {
        "id": "mrZ_uDdyIkgj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts = ['text', 'the', 'leader', 'prime', 'natural', 'language']"
      ],
      "metadata": {
        "id": "C3IlsWozINTU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and fit tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)"
      ],
      "metadata": {
        "id": "TFIsPrWYI4G0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)"
      ],
      "metadata": {
        "id": "WeuoFwY3MxXZ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.word_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTDrnT24M-Fw",
        "outputId": "81660e9b-e11f-44cc-c322-3398e5555cfa"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': 1, 'the': 2, 'leader': 3, 'prime': 4, 'natural': 5, 'language': 6}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Output the word-index dicctionary\n",
        "print(\"Number of unique words in dicitonary =\", len(tokenizer.word_index))\n",
        "print(\"Dictinary is =\", tokenizer.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6KsayoJJOiC",
        "outputId": "1304700e-3a9a-437e-9663-5ad57b0fb754"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique words in dicitonary = 6\n",
            "Dictinary is = {'text': 1, 'the': 2, 'leader': 3, 'prime': 4, 'natural': 5, 'language': 6}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLC3b_ZML2oW",
        "outputId": "d05c4dc0-f6d2-440f-90e1-f1de4f8b0b7d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': 1, 'the': 2, 'leader': 3, 'prime': 4, 'natural': 5, 'language': 6}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.word_index['text']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_NeDBdFcjwW",
        "outputId": "ef243aff-244f-4bae-abcd-10c16baf8d3a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(tokenizer.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_i1nUYXwMsVV",
        "outputId": "1bb0e880-e982-4645-bb54-7c3c5cbb8e20"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function to create embedding matrix\n",
        "# define function that loads Glove word vectors from file\n",
        "# creates an embedding matrix matching tokenizer word indices with GloVe vectors"
      ],
      "metadata": {
        "id": "NEnz4VqWNBZ5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def embedding_for_vocab(filepath, word_index, embedding_dim):\n",
        "  vocab_size = len(word_index) + 1\n",
        "  embedding_matrix_vocab = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "  with open(filepath, encoding='utf8') as f:\n",
        "    for line in f:\n",
        "      word, *vector = line.split()"
      ],
      "metadata": {
        "id": "63ydUH2hP2_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def embedding_for_vocab(filepath, word_index, embedding_dim):\n",
        "  vocab_size = len(word_index) + 1\n",
        "  embedding_matrix_vocab = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "  with open(filepath, encoding='utf8') as f:\n",
        "    for line in f:\n",
        "      word, *vector = line.split()  # word and vocabulary index splitting\n",
        "      if word in word_index:  # extract index if our word indexing matches dict dimension of GloVe\n",
        "        idx = word_index[word] # idx is vocabulary indexing in dictionary\n",
        "        embedding_matrix_vocab[idx] = np.array(vector, dtype=np.float32)[:embedding_dim]  # insert that vector into each row\n",
        "  return embedding_dim"
      ],
      "metadata": {
        "id": "x4suyR_CNubg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "emebdding_size = 6\n",
        "\n"
      ],
      "metadata": {
        "id": "1k4tIS_afooz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def embedding_for_vocab(filepath, word_index, embedding_dim):\n",
        "  vocab_size = len(word_index) + 1\n",
        "  print(vocab_size)"
      ],
      "metadata": {
        "id": "Nmh6UKhhfa1z"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.zeros((5, 7))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyCKab_Zc7DZ",
        "outputId": "3c489bdd-5d28-4a78-a23c-1e2ed7710ba7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_zeros = np.zeros((5, 7))\n",
        "all_zeros[0] = np.array(np.ones(7) , dtype=np.float32)\n",
        "\n",
        "\n",
        "all_zeros\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mnhJHSXc9Kt",
        "outputId": "c1834aab-3762-4345-ce02-505f913f50f2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 1., 1., 1., 1., 1., 1.],\n",
              "       [0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the GloVe dataset\n",
        "!wget https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
        "\n",
        "# Unzip the file\n",
        "!unzip -q glove.6B.zip\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85skIP7ydeAq",
        "outputId": "6c0a777e-6910-4aa0-b6a9-556728610d6d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-31 11:36:37--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.02MB/s    in 2m 39s  \n",
            "\n",
            "2025-07-31 11:39:17 (5.16 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set the embedding dimension\n",
        "embedding_dim = 50\n",
        "\n",
        "# path to Glove file\n",
        "glove_path = './glove.6B.50d.txt'\n",
        "\n",
        "embedding_matrix_vocab = embedding_for_vocab(glove_path, tokenizer.word_index, embedding_dim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPHfxrkBRRYC",
        "outputId": "82ea4990-c03b-4301-d677-8259e53487b0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print the dense vector for the first word in the tokenizer index\n",
        "first_word_index = 1\n",
        "print(\"Dense vector for word with index 1 => \", embedding_matrix_vocab[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "4zfbDJ4JUG6e",
        "outputId": "b61da1c7-4a31-4fed-97a3-ae0fa3c105db"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'NoneType' object is not subscriptable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2667344958.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# print the dense vector for the first word in the tokenizer index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfirst_word_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Dense vector for word with index 1 => \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_matrix_vocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "E5AgVEuXUoo2"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = ['I', 'am', 'watching', 'anime', 'named', 'dan', 'da', 'dan', 'season', '2']\n"
      ],
      "metadata": {
        "id": "zgyLSwpojMcH"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(text)\n",
        "sequences = tokenizer.texts_to_sequences(text)\n",
        "print('embedding and dictionary:', tokenizer.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SopCQTbAjv5X",
        "outputId": "f8105731-7b33-4853-dff6-2fad60445ba2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "embedding and dictionary: {'dan': 1, 'i': 2, 'am': 3, 'watching': 4, 'anime': 5, 'named': 6, 'da': 7, 'season': 8, '2': 9}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Number of unique words in the dictionary:', len(tokenizer.word_index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnJEEP_JlM_p",
        "outputId": "4b69ce4f-e449-496a-fabb-5d2717e220eb"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique words in the dictionary: 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's define the GloVe words vector from file\n",
        "# Creates an embedding matrix matching tokenizer word\n",
        "def embedding_for_vocab(file_path, word_index, dim):\n",
        "  vocab_size = len(word_index) + 1 # for padding token index 0\n",
        "  embedding_matrix_vocab = np.zeros((vocab_size, dim))\n",
        "\n",
        "  with open(file_path, encoding='utf8') as f:\n",
        "    for line in f:\n",
        "      word, *vector = line.split()\n",
        "      if word in word_index:\n",
        "        idx = word_index[word]\n",
        "        embedding_matrix_vocab[idx] = np.array(vector, dtype=np.float32)[:]\n",
        "\n",
        "    return embedding_matrix_vocab"
      ],
      "metadata": {
        "id": "xLHDnaO5l2jL"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set the embedding dimension\n",
        "embedding_dim = 50\n",
        "\n",
        "# path to Glove file\n",
        "glove_path = './glove.6B.50d.txt'\n",
        "\n",
        "embedding_matrix_vocab = embedding_for_vocab(glove_path, tokenizer.word_index, embedding_dim)"
      ],
      "metadata": {
        "id": "GCd79MTNwRtm"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix_vocab = embedding_for_vocab(glove_path, tokenizer.word_index, embedding_dim)"
      ],
      "metadata": {
        "id": "WiQnY2jJvoNk"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_matrix_vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQ4rmGb70FYG",
        "outputId": "39b67dc0-50c7-49b7-9cf7-7bb0e0dde8a1"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00]\n",
            " [-1.61689997e+00  2.10620001e-01  6.71169996e-01  1.08879995e+00\n",
            "  -1.59799993e-01 -3.10889989e-01 -1.03240001e+00 -6.69210032e-02\n",
            "  -8.07539999e-01 -1.11240005e+00 -3.04280013e-01  6.55330002e-01\n",
            "  -4.98560011e-01 -1.04240000e-01  6.38599992e-01  9.70339999e-02\n",
            "   3.36510003e-01 -5.69350004e-01  1.10860001e-02  4.30200011e-01\n",
            "  -4.60689992e-01 -5.62669992e-01  3.12220007e-01  1.43110007e-01\n",
            "   5.87859988e-01 -1.27970004e+00  1.62699997e-01 -5.83440006e-01\n",
            "  -3.34039986e-01 -4.94619995e-01  1.45609999e+00 -1.55959994e-01\n",
            "   3.48190010e-01 -2.28949994e-01 -3.29530001e-01  6.41799986e-01\n",
            "   1.26690000e-01  2.76219994e-02  5.09739995e-01  3.65590006e-01\n",
            "   2.05650002e-01  1.33089995e+00 -9.21720028e-01 -4.97850001e-01\n",
            "   7.00659990e-01 -5.33079982e-01  2.16159999e-01 -4.52899992e-01\n",
            "  -1.66539997e-01  1.57050002e+00]\n",
            " [ 1.18910000e-01  1.52549997e-01 -8.20730031e-02 -7.41439998e-01\n",
            "   7.59169996e-01 -4.83280003e-01 -3.10090005e-01  5.14760017e-01\n",
            "  -9.87079978e-01  6.17570011e-04 -1.50429994e-01  8.37700009e-01\n",
            "  -1.07969999e+00 -5.14599979e-01  1.31879997e+00  6.20069981e-01\n",
            "   1.37789994e-01  4.71080005e-01 -7.28740022e-02 -7.26750016e-01\n",
            "  -7.41159976e-01  7.52629995e-01  8.81799996e-01  2.95610011e-01\n",
            "   1.35479999e+00 -2.57010007e+00 -1.35230005e+00  4.58799988e-01\n",
            "   1.00680006e+00 -1.18560004e+00  3.47370005e+00  7.78980017e-01\n",
            "  -7.29290009e-01  2.51020014e-01 -2.61559993e-01 -3.46839994e-01\n",
            "   5.58409989e-01  7.50980020e-01  4.98299986e-01 -2.68229991e-01\n",
            "  -2.74430006e-03 -1.82980001e-02 -2.80959994e-01  5.53179979e-01\n",
            "   3.77059989e-02  1.85550004e-01 -1.50250003e-01 -5.75119972e-01\n",
            "  -2.66710013e-01  9.21209991e-01]\n",
            " [ 3.46639991e-01  3.98050010e-01  4.89699990e-01 -5.14209986e-01\n",
            "   5.45740008e-01 -1.20050001e+00  3.21069986e-01  7.40040004e-01\n",
            "  -1.49790001e+00 -1.96510002e-01 -1.26310006e-01 -3.77029985e-01\n",
            "  -6.25689983e-01  3.87919992e-02  1.05789995e+00  7.71990001e-01\n",
            "  -1.85890004e-01  1.30320001e+00 -7.21279979e-01  4.02310014e-01\n",
            "   6.64419979e-02  1.23150003e+00  9.39559996e-01  1.39030004e+00\n",
            "   1.53340006e+00 -1.47300005e+00 -3.49970013e-01  3.15620005e-01\n",
            "   9.06910002e-01  4.54979986e-01  2.54809999e+00  1.64100006e-01\n",
            "  -6.06999993e-01  2.70610005e-01 -7.90719986e-01 -1.14600003e+00\n",
            "   9.17949975e-01 -1.17969997e-01  2.35259995e-01 -1.26589999e-01\n",
            "   6.65269971e-01 -9.18160021e-01  1.00479998e-01  7.04569995e-01\n",
            "  -2.17769995e-01  5.24789989e-01 -5.44520020e-01  8.65759999e-02\n",
            "   3.40369999e-01  1.35880005e+00]\n",
            " [-4.90870001e-03  1.26110002e-01  1.40560001e-01 -1.64179996e-01\n",
            "   6.21050000e-01 -8.58449996e-01 -1.00650001e+00  2.67699987e-01\n",
            "  -1.11639999e-01 -1.53379999e-02 -4.08219993e-01 -2.27780007e-02\n",
            "  -2.48089999e-01  7.70780027e-01  1.20169997e+00  4.06320006e-01\n",
            "   6.40669987e-02  3.71679991e-01 -7.57910013e-01 -7.95970023e-01\n",
            "   2.46800005e-01  9.90760028e-01  4.43019986e-01  5.57630002e-01\n",
            "   9.46179986e-01 -1.27400005e+00 -3.69899988e-01  5.22469997e-01\n",
            "   5.14959991e-01 -6.23799980e-01  2.14359999e+00  1.17019999e+00\n",
            "   2.06799999e-01 -6.13020003e-01 -4.31100011e-01  2.33400002e-01\n",
            "  -1.73360005e-01  3.35999988e-02 -2.97950000e-01 -4.84710008e-01\n",
            "  -5.84619999e-01  4.26230013e-01 -4.21519995e-01  4.24670011e-01\n",
            "   4.58999991e-01 -4.10009995e-02  2.06949994e-01 -5.84640026e-01\n",
            "  -3.18309993e-01  3.15640002e-01]\n",
            " [ 4.73250002e-02 -1.61899999e-01 -1.23090005e+00  1.13360000e+00\n",
            "  -2.50550002e-01  5.56689978e-01 -3.22899997e-01 -1.14059997e+00\n",
            "  -4.70200002e-01  8.41719985e-01  6.53180003e-01  9.76339996e-01\n",
            "   6.66590035e-02  1.27010000e+00  7.75900006e-01 -8.35810006e-01\n",
            "  -9.20340002e-01  1.10169995e+00 -1.13929999e+00  4.19200003e-01\n",
            "   1.06110001e+00 -5.22249997e-01  8.10469985e-01  1.08449996e+00\n",
            "   4.41249996e-01  2.19730005e-01 -1.08679998e+00  1.00730002e-01\n",
            "  -5.41419983e-01 -1.00720000e+00  1.09410000e+00 -7.30869994e-02\n",
            "   7.52650023e-01 -5.48110008e-01 -3.38550001e-01  2.18710005e-01\n",
            "   2.13450000e-01 -9.79830027e-01 -2.01069999e+00 -5.72969973e-01\n",
            "  -8.38669986e-02 -3.93339992e-01 -8.21600020e-01 -9.70170021e-01\n",
            "   4.05859984e-02 -1.60869993e-02  6.38480008e-01  1.17030004e-02\n",
            "   1.09820001e-01  1.33599997e-01]\n",
            " [-1.52230002e-02  9.47910011e-01 -3.97980005e-01  2.42449999e-01\n",
            "   8.37419987e-01  2.18920007e-01 -1.49080002e+00 -1.47719994e-01\n",
            "  -1.72700003e-01 -8.14610004e-01  5.31310022e-01  9.22169983e-01\n",
            "  -9.24070030e-02  2.37489998e-01  1.65900007e-01 -1.12480000e-01\n",
            "   6.08580001e-02  7.74749994e-01 -6.85590029e-01  5.98919988e-01\n",
            "  -5.64880013e-01 -8.66900012e-02 -1.36449993e-01  4.46720004e-01\n",
            "  -1.16729997e-01 -1.84850001e+00 -6.69749975e-01 -1.76880002e-01\n",
            "  -9.73510027e-01 -2.23789997e-02  1.97609997e+00 -1.12950003e+00\n",
            "  -3.34910005e-01 -3.60630006e-01  7.08580017e-01  1.36329994e-01\n",
            "   1.29899994e-01  2.09059998e-01  1.27250001e-01 -3.73199999e-01\n",
            "  -5.97480014e-02  3.85159999e-01 -5.85850000e-01 -7.37320006e-01\n",
            "  -4.01670001e-02 -1.79910004e-01 -6.36319995e-01 -9.09150004e-01\n",
            "   3.29659998e-01  3.46729994e-01]\n",
            " [ 8.94540012e-01 -2.85930008e-01 -4.92499992e-02 -5.59469983e-02\n",
            "  -7.51890004e-01 -1.62600005e+00 -3.15380007e-01 -7.55429983e-01\n",
            "  -1.40330005e+00  1.06550002e+00  2.54440010e-01  8.71119976e-01\n",
            "  -1.10229999e-01 -7.61059999e-01 -1.04869999e-01 -1.26329994e+00\n",
            "  -4.99069989e-01  3.50730002e-01  2.34190002e-02 -7.77439997e-02\n",
            "  -1.02719998e+00 -8.34190026e-02  1.05149997e-02  4.31580007e-01\n",
            "   5.40650010e-01 -4.22100008e-01 -3.95920008e-01  9.43299979e-02\n",
            "  -1.57000005e+00 -9.37269986e-01  1.82650006e+00 -4.13590014e-01\n",
            "  -1.07270002e+00  3.55149992e-02  4.34659988e-01 -1.47630006e-01\n",
            "   3.03739995e-01 -2.06560001e-01  7.72509992e-01 -4.51440006e-01\n",
            "  -1.29920006e-01  4.24890012e-01 -1.83329999e-01 -1.80200005e+00\n",
            "   5.17779998e-02  4.56000008e-02  5.01330018e-01 -9.24220026e-01\n",
            "   2.15339994e+00 -1.90629996e-02]\n",
            " [-1.03859997e+00  5.23199975e-01 -7.31410027e-01  2.21990004e-01\n",
            "  -3.58429998e-01 -1.60970002e-01 -1.78820002e+00  3.44590008e-01\n",
            "  -4.80960011e-01 -1.58439994e-01  2.14100003e-01  1.00960001e-01\n",
            "  -1.50510001e+00 -2.33549997e-02  1.70330000e+00 -4.88810003e-01\n",
            "   3.14499997e-02 -1.63980007e-01 -2.29329991e+00  3.32309991e-01\n",
            "  -7.90109992e-01 -8.72399986e-01  8.67380023e-01 -6.44299984e-02\n",
            "  -3.97359997e-01 -4.92689997e-01  9.10150036e-02  1.68070003e-01\n",
            "   7.27699995e-01 -1.72429994e-01  3.03620005e+00  1.62769997e+00\n",
            "   2.35009998e-01 -1.03689998e-01  6.00740016e-01  7.19330013e-01\n",
            "   4.40860003e-01  1.23889995e+00  1.59290001e-01 -1.21270001e+00\n",
            "  -1.25409997e+00 -7.43200004e-01 -1.01080000e+00 -3.67780000e-01\n",
            "  -8.63099992e-01  7.92320013e-01  6.31280005e-01  1.87910005e-01\n",
            "  -2.48009991e-02  4.24109995e-01]\n",
            " [-1.10979997e-01  8.67240012e-01  7.81140029e-01  6.29270017e-01\n",
            "   4.74440008e-01  5.69949985e-01 -3.65890004e-02 -3.62769991e-01\n",
            "  -7.58350015e-01 -2.31769994e-01 -3.38630006e-02 -1.37989998e-01\n",
            "  -2.73319989e-01 -4.92680013e-01  6.53039992e-01 -6.58739984e-01\n",
            "  -3.75979990e-01 -3.51749994e-02 -1.75100005e+00  4.25099999e-01\n",
            "   2.78230011e-01 -7.04689980e-01  1.43089998e+00  4.74070013e-01\n",
            "  -7.92699993e-01 -5.93439996e-01  7.27970004e-01 -4.67629999e-01\n",
            "   4.27509993e-01 -5.45570016e-01  3.60549998e+00  3.72299999e-01\n",
            "  -4.93939996e-01  7.21300006e-01  3.86480004e-01 -1.20739996e-01\n",
            "   5.22769988e-01  1.10909998e-01  7.68809974e-01 -7.13680029e-01\n",
            "   7.36639977e-01 -5.56400001e-01  5.65379977e-01 -1.15649998e+00\n",
            "  -3.93940002e-01  1.30170000e+00  2.82810003e-01 -6.17519975e-01\n",
            "   5.91030002e-01  2.86489993e-01]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "first_word_index = 1\n",
        "print(\"Dense vector for word with index 1 = \", embedding_matrix_vocab[first_word_index])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVAgkccX0JdV",
        "outputId": "4bedc8b9-6227-4124-8b19-0d3c05ad003b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dense vector for word with index 1 =  [-1.61689997  0.21062     0.67117     1.08879995 -0.15979999 -0.31088999\n",
            " -1.03240001 -0.066921   -0.80754    -1.11240005 -0.30428001  0.65533\n",
            " -0.49856001 -0.10424     0.63859999  0.097034    0.33651    -0.56935\n",
            "  0.011086    0.43020001 -0.46068999 -0.56266999  0.31222001  0.14311001\n",
            "  0.58785999 -1.27970004  0.1627     -0.58344001 -0.33403999 -0.49462\n",
            "  1.45609999 -0.15595999  0.34819001 -0.22894999 -0.32953     0.64179999\n",
            "  0.12669     0.027622    0.50974     0.36559001  0.20565     1.33089995\n",
            " -0.92172003 -0.49785     0.70065999 -0.53307998  0.21616    -0.45289999\n",
            " -0.16654     1.57050002]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BufenJrE02-1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}